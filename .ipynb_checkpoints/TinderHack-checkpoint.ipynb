{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan de trabajo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **6 de marzo: Bot que pueda dar likes con base a belleza**\n",
    "* **10 de marzo: Análisis de sentimiento de las descripciones**\n",
    "* 13 de marzo: Acceder a cookies y sacar info. relevante para modelo de la persona / **ENTRENAR CON MATCHES EXISTENTES**\n",
    "* 17 de marzo: Conectar todo\n",
    "* 20 de marzo: Tener lista formas de mejorar el modelo y que vaya aprendiendo con retro del usuario\n",
    "* 23 de marzo: Carpeta que arroje los insights más importantes\n",
    "* 24 de marzo: Presentación\n",
    "* 25 de marzo: Script\n",
    "* 26 de marzo: Ensayo \n",
    "\n",
    "Empezar entrenando con matches existentes, guardándolos en una base de datos, nombre, edad, descripción, km distancia, ubicación ,etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fotos proteger\n",
    "#nombres tokenizados con descripciones en formato json\n",
    "#implicitly wait\n",
    "#collab: run time a gpu\n",
    "#frases distancia coseno descripción matches que gustan a ti\n",
    "#levenstein\n",
    "#preclasificar embudo ver inst desc\n",
    "\n",
    "#descargar todas las imagenes en un folder por persona con un token especifico\n",
    "#ligar esa columna para un ovr score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASS SCRAPING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.display import Image, display, clear_output\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "import pathlib\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib\n",
    "from time import sleep\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from random import randint\n",
    "from secrets import username, password\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from datetime import datetime\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.image as mpimg\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import itertools\n",
    "from googletrans import Translator\n",
    "import emoji\n",
    "import flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_seq_items = 1000\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\"\n",
    "\n",
    "work = 'M7.15 3.434h5.7V1.452a.728.728 0 0 0-.724-.732H7.874a.737.737 0 0 0-.725.732v1.982z'\n",
    "education = 'M11.87 5.026L2.186 9.242c-.25.116-.25.589 0 .705l.474.204v2.622a.78.78 0 0 0-.344.657c0 .42.313.767.69.767.378 0 .692-.348.692-.767a.78.78 0 0 0-.345-.657v-2.322l2.097.921a.42.42 0 0 0-.022.144v3.83c0 .45.27.801.626 1.101.358.302.842.572 1.428.804 1.172.46 2.755.776 4.516.776 1.763 0 3.346-.317 4.518-.777.586-.23 1.07-.501 1.428-.803.355-.3.626-.65.626-1.1v-3.83a.456.456 0 0 0-.022-.145l3.264-1.425c.25-.116.25-.59 0-.705L12.13 5.025c-.082-.046-.22-.017-.26 0v.001zm.13.767l8.743 3.804L12 13.392 3.257 9.599l8.742-3.806zm-5.88 5.865l5.75 2.502a.319.319 0 0 0 .26 0l5.75-2.502v3.687c0 .077-.087.262-.358.491-.372.29-.788.52-1.232.68-1.078.426-2.604.743-4.29.743s-3.212-.317-4.29-.742c-.444-.161-.86-.39-1.232-.68-.273-.23-.358-.415-.358-.492v-3.687z'\n",
    "gender = 'M15.507 13.032c1.14-.952 1.862-2.656 1.862-5.592C17.37 4.436 14.9 2 11.855 2 8.81 2 6.34 4.436 6.34 7.44c0 3.07.786 4.8 2.02 5.726-2.586 1.768-5.054 4.62-4.18 6.204 1.88 3.406 14.28 3.606 15.726 0 .686-1.71-1.828-4.608-4.4-6.338'\n",
    "cols = ['name','age', 'number_of_photos', 'education', 'work', 'sexual_orientation', 'location','km_distance','similar_likes','other_likes','description','verified','super_liked_me','instagram','spotify','artist','song', 'attractiveness_score', 'description_score', 'folder']\n",
    "\n",
    "attractiveness_model = load_model(\"ratings.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class colors():\n",
    "    \n",
    "    PURPLE = '\\033[95m'\n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'\n",
    "\n",
    "class TinderBot():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.options = webdriver.ChromeOptions()\n",
    "        self.options.add_argument('--incognito')\n",
    "        self.options.add_argument(\"--disable-infobars\")\n",
    "        self.options.add_argument(\"start-maximized\")\n",
    "        self.options.add_argument(\"--disable-extensions\")\n",
    "        self.options.add_argument(\"--disable-popup-blocking\")\n",
    "\n",
    "        prefs = {\n",
    "            'profile.default_content_setting_values':\n",
    "            {\n",
    "                'notifications': 1,\n",
    "                'geolocation': 1\n",
    "            },\n",
    "\n",
    "            'profile.managed_default_content_settings':\n",
    "            {\n",
    "                'geolocation': 1\n",
    "            },\n",
    "        }\n",
    "\n",
    "        self.options.add_experimental_option('prefs', prefs)\n",
    "        self.driver = webdriver.Chrome('/Applications/Avid/Licenses/Licence/Data_Science/Ironhack/Data_Analytics/chromedriver', options=self.options)\n",
    "    \n",
    "    def login(self):\n",
    "        self.driver.get('https://tinder.com')\n",
    "\n",
    "        assert \"Tinder\" in self.driver.title\n",
    "\n",
    "        sleep(randint(1,5))\n",
    "        log_in = self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div/div/header/div/div[2]/div[2]/button')\n",
    "        log_in.click()\n",
    "\n",
    "        sleep(randint(1,5))\n",
    "\n",
    "        self.driver.switch_to_alert\n",
    "\n",
    "        sleep(randint(1,5))\n",
    "\n",
    "        fb_button = self.driver.find_element_by_xpath('.//div/div/div[1]/div/div[3]/span/div[2]/button/span[2]')\n",
    "        fb_button.click()\n",
    "\n",
    "        window_before = self.driver.window_handles[0]\n",
    "        window_after = self.driver.window_handles[1]\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        self.driver.switch_to.window(window_after)\n",
    "\n",
    "        user_in = self.driver.find_element_by_xpath('//*[@id=\"email\"]')\n",
    "        user_in.send_keys(username)\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        pass_in = self.driver.find_element_by_xpath('//*[@id=\"pass\"]')\n",
    "        pass_in.send_keys(password)\n",
    "\n",
    "        login_btn = self.driver.find_element_by_xpath('/html/body/div/div[2]/div[1]/form/div/div[3]/label[2]/input')\n",
    "        login_btn.click()\n",
    "\n",
    "        self.driver.switch_to.window(window_before)\n",
    "\n",
    "        sleep(5)\n",
    "\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div/div/div/div[3]/button[1]/span').click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('/html/body/div[2]/div/div/div/div/div[3]/button[1]').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        sleep(1.2)\n",
    "\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div/div/div/div[3]/button[1]').click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('//*[@id=\"t--1610880557\"]/div/div/div/div/div[3]/button[1]').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        sleep(1.6)\n",
    "\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div[2]/div/div/div[1]/button').click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('/html/body/div[1]/div/div[2]/div/div/div[1]/button').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def info_button(self):\n",
    "        \n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div[1]/div/div[1]/div[3]/div[3]/button').click()\n",
    "        except:\n",
    "            try:\n",
    "                WebDriverWait(self.driver, 4).until(EC.element_to_be_clickable((By.XPATH, \"/html/body/div[1]/div/div[1]/div/div/main/div/div[1]/div/div[1]/div[3]/div[3]/button\"))).click()\n",
    "            except:\n",
    "                try:\n",
    "                    self.driver.find_element_by_xpath(\"//div[contains(.,'Open Profile')]\").click()\n",
    "                except:\n",
    "                    try:\n",
    "                        self.driver.find_element_by_xpath(\"//div[contains(.,'Open Profile')]\").click()\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "    def to_photo(self):\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(self.driver,4).until(EC.element_to_be_clickable((By.XPATH, './/div/div[1]/div/main/div[1]/div/div/div[1]'\n",
    "    ))).click()\n",
    "        except:\n",
    "            try:\n",
    "                WebDriverWait(self.driver,4).until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div/div[1]/div/main/div[1]/div/div/div[1]'))).click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def to_main(self):\n",
    "        \n",
    "        try:\n",
    "            WebDriverWait(self.driver,4).until(EC.element_to_be_clickable((By.XPATH, './/div/div[1]/div/main'))).click()\n",
    "        except:\n",
    "            try:\n",
    "                WebDriverWait(self.driver,4).until(EC.element_to_be_clickable((By.XPATH, '/html/body/div[1]/div/div[1]/div/main'))).click()\n",
    "            except:\n",
    "                try:\n",
    "                    self.driver.find_element_by_xpath(\".//div/div[1]/div/main\").click()\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    def back_to_swipe(self):\n",
    "        \n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div/div[1]/div/div/div[1]/a/button').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def get_name(self, soup):\n",
    "        \n",
    "        try:\n",
    "            name = soup.h1.string\n",
    "            return name\n",
    "        except:\n",
    "            try:\n",
    "                name = soup.find(\"span\", itemprop=\"name\").text\n",
    "                return name\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def get_age(self, soup):\n",
    "        \n",
    "        try:\n",
    "            age = soup.select('div [class=\"Whs(nw) Fz($l)\"]')[0].text\n",
    "            return age\n",
    "        except:\n",
    "            try:\n",
    "                age = soup.find(\"span\", itemprop=\"age\").text\n",
    "                return age\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def get_info(self, soup):\n",
    "        \n",
    "        try:\n",
    "            info = soup.select('div [class=\"Row\"]')\n",
    "            all_info = [info[i].text for i in range(len(info))]\n",
    "            return all_info\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "    def get_description(self, soup):\n",
    "        \n",
    "        try:\n",
    "            description = soup.select('div [class=\"P(16px) Ta(start) Us(t) C($c-secondary) BreakWord Whs(pl) Fz($ms)\"]')[0].text\n",
    "            return description\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def get_likes(self, soup):\n",
    "        \n",
    "        try:\n",
    "            lista = soup.select('div [class=\"Bdrs(100px) Bd D(ib) Fz($xs) Px(8px) Py(4px) Mend(4px) Mend(8px) Mb(8px) Mb(4px)--s Bdc($c-secondary) C($c-secondary)\"]')\n",
    "            if len(lista) > 0:\n",
    "                all_likes = ', '.join([lista[i].text.strip().lower() for i in range(len(lista))])\n",
    "                return all_likes\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            pass           \n",
    "        \n",
    "    def get_similar_likes(self, soup):\n",
    "        \n",
    "        try:\n",
    "            lista = soup.select('div [class=\"Bdrs(100px) Bd D(ib) Fz($xs) Px(8px) Py(4px) Mend(4px) Mend(8px) Mb(8px) Mb(4px)--s Bdc($c-pink) C($c-pink)\"]')\n",
    "            if len(lista) > 0:\n",
    "                all_similar_likes = ', '.join([lista[i].text.strip().lower() for i in range(len(lista))])\n",
    "                return all_similar_likes\n",
    "            else:\n",
    "                return None\n",
    "        except:\n",
    "            pass   \n",
    "\n",
    "    def get_photo_len(self, soup):\n",
    "        \n",
    "        try:\n",
    "            photo_len = [int(i.text.strip()[-1]) for i in soup.select('div[class=\"CenterAlign D(f) Fxd(r) W(100%) Px(8px) Pos(a) TranslateZ(0)\"]')][0]\n",
    "            return photo_len\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    def scrap_photos(self, photo_len):\n",
    "\n",
    "        links = []\n",
    "\n",
    "        for i in range(photo_len):\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath(f'.//div/div[1]/div/main/div[1]/div/div/div/div[2]/div/div[1]/div/div/div[1]/span/div/div[2]/button[{i+1}]').click()\n",
    "            except:\n",
    "                try:\n",
    "                    self.driver.find_element_by_xpath(f'/html/body/div[1]/div/div[1]/div/main/div[1]/div/div/div[1]/div[1]/div/div[1]/span/div/div[2]/button[{i+1}]').click()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            sleep(2.5)\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "            sleep(0.5)\n",
    "            photos = soup.select('div[class=\"profileCard__slider__img Z(-1)\"]')\n",
    "            sleep(0.1)\n",
    "            sub_links = [str(photos[i]).split('(\"', 1)[1].split('\")')[0] for i in range(len(photos))]\n",
    "            links.append(sub_links)\n",
    "\n",
    "        flat_list = list(set([item for sublist in links for item in sublist]))\n",
    "        return flat_list\n",
    "\n",
    "    def save_images(self, number, tag):\n",
    "        \n",
    "        path = f'{os.getcwd()}/images/matches/{tag}'\n",
    "        os.mkdir(path)\n",
    "        \n",
    "        for i, url in enumerate(number):\n",
    "\n",
    "            save_webp = f'{path}/{i}-{tag}.webp'\n",
    "            save_jpg = f'{path}/{i}-{tag}.jpg'\n",
    "\n",
    "            if re.findall(r'([^.]*)$', url)[0] == 'webp':\n",
    "                PIL.Image.open(f'{urllib.request.urlretrieve(url, save_webp)[0]}').convert(\"RGB\").save(save_jpg,\"jpeg\")\n",
    "                sleep(0.5)\n",
    "                os.remove(save_webp)\n",
    "            else:\n",
    "                urllib.request.urlretrieve(url, save_jpg)\n",
    "\n",
    "    def get_spotify(self, soup):\n",
    "        \n",
    "        try:\n",
    "            h2_list = [i.text.strip().lower() for i in soup.select('div [class=\"P(16px)\"] > h2')]\n",
    "            if 'my anthem' in h2_list:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def get_artist(self, soup):\n",
    "        \n",
    "        artist = soup.select('div [class=\"D(f) Fz($s) C($c-secondary)\"]')[0].text\n",
    "        return artist\n",
    "\n",
    "    def get_song(self, soup):\n",
    "        \n",
    "        song = soup.select('div [class=\"Mb(4px) Ell Fz($ms)\"]')[0].text\n",
    "        return song\n",
    "\n",
    "    def get_instagram(self, soup):\n",
    "        \n",
    "        try:\n",
    "            h2_list = [i.text.strip().lower() for i in soup.select('div [class=\"P(16px)\"] > h2')]\n",
    "            if 'recent instagram photos' in h2_list:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    def plot_images(self, number, tag):\n",
    "        \n",
    "        images = []\n",
    "        path = f'{os.getcwd()}/images/matches/{tag}'\n",
    "\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            for img_path in files:\n",
    "                if img_path.endswith('.jpg'):\n",
    "                    images.append(mpimg.imread(f'{path}/{img_path}'))\n",
    "\n",
    "        plt.figure(figsize=(10,5))\n",
    "        columns = number\n",
    "\n",
    "        for i, image in enumerate(images):\n",
    "            plt.subplot(int(len(images)/columns+1), columns, i+1)\n",
    "            plt.imshow(image)\n",
    "            plt.gca().axes.get_yaxis().set_visible(False)\n",
    "            plt.gca().axes.get_xaxis().set_visible(False)\n",
    "        plt.show()\n",
    "     \n",
    "    def verified(self, soup):\n",
    "        \n",
    "        try:\n",
    "            icons = soup.select('div [class=\"D(ib) Lh(0) Sq(30px) Mstart(4px) As(c)\"]')\n",
    "            if len(icons) > 0:\n",
    "                for i in range(len(icons)):\n",
    "                    if 'Verified!' in icons[i].text:\n",
    "                        return True\n",
    "                    else:\n",
    "                        return False\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def super_like(self, soup):\n",
    "        try:\n",
    "            icons = soup.select('div [class=\"D(ib) Lh(0) Sq(30px) Mstart(4px) As(c)\"]')\n",
    "            if len(icons) > 0:\n",
    "                for i in range(len(icons)):\n",
    "                    if 'Super Like' in icons[i].text:\n",
    "                        return True\n",
    "                    else:\n",
    "                        return False\n",
    "            else:\n",
    "                return False\n",
    "        except:\n",
    "            pass          \n",
    "\n",
    "    def delete_images(self, tag):\n",
    "        \n",
    "        path = f'{os.getcwd()}/images/{tag}'\n",
    "        os.rmdir(path)\n",
    "\n",
    "    def info_out(self):\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('//a[@class=\"End(12px) B(-20px) Pos(a) Z(2) CenterAlign Bdrs(50%) P(0) Scale(1.1):h Trsdu($normal) focus-button-style\"]/*[name()=\"svg\"]/*[name()=\"g\"]').click()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def like_button(self):\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath(\".//div/div[1]/div/main/div[1]/div/div/div[1]/div[2]/div/div/div[4]/button\").click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div[1]/div/div[2]/div[4]/button').click()\n",
    "            except:\n",
    "                try:\n",
    "                    self.driver.find_element_by_css_selector('#t--1032254752 > div > div.App__body.H\\(100\\%\\).Pos\\(r\\).Z\\(0\\) > div > div > main > div > div.recsCardboard.W\\(100\\%\\).Mt\\(a\\).H\\(100\\%\\)--s.Px\\(4px\\)--s.Pos\\(r\\) > div > div.Pos\\(r\\).Py\\(16px\\).Py\\(12px\\)--s.Px\\(4px\\).Px\\(8px\\)--ml.D\\(f\\).Jc\\(sb\\).Ai\\(c\\).Maw\\(375px\\)--m.Mx\\(a\\).Pe\\(n\\).Mt\\(-1px\\) > div:nth-child(4) > button > span > svg').click()\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    def dislike_button(self):\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div[1]/div[2]/div/div/div[2]/button').click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('.//div/div[1]/div/main/div[1]/div/div/div[1]/div/div[2]/div[2]/button').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def close_match(self):\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath('//*[@title=\"Back to Tinder\"]').click()\n",
    "        except:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('.//div/div/div[1]/div/div[4]/button').click()\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    def no_super_like(self):\n",
    "        try:\n",
    "            self.driver.find_element_by_xpath(\"/html/body/div[2]/div/div/button[2]\").click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def matches_button(self):\n",
    "        try:\n",
    "            mtchs_btn = self.driver.find_element_by_xpath('.//div/div[1]/div/aside/nav/div/div/div/div[1]/div/div[1]')\n",
    "            mtchs_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def get_number_of_matches(self):\n",
    "        \n",
    "        self.matches_button()\n",
    "        \n",
    "        actions = ActionChains(self.driver)\n",
    "        m = 0 \n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('.//div[1]/div[2]/a/div[1]/div').click()\n",
    "                sleep(2)\n",
    "                self.driver.find_element_by_xpath('.//div[1]/div[2]/a/div[1]/div').click()\n",
    "                sleep(2)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            actions.send_keys(Keys.END)\n",
    "            actions.perform()\n",
    "            sleep(2)\n",
    "\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "            sleep(5)\n",
    "            number_of_matches = len(soup.select('div [class=\"P(8px)\"]'))\n",
    "\n",
    "            if (number_of_matches != m):\n",
    "                m = number_of_matches\n",
    "            else:\n",
    "                return number_of_matches\n",
    "                break\n",
    "                \n",
    "        self.back_to_swipe()\n",
    "          \n",
    "    def message_button(self):\n",
    "        try:\n",
    "            msg_btn = self.driver.find_element_by_xpath('.//div/div[1]/div/aside/nav/div/div/div/div[1]/div/div[2]')\n",
    "            msg_btn.click()\n",
    "        except:\n",
    "            pass\n",
    "                \n",
    "    def get_number_of_messages(self):\n",
    "    \n",
    "        self.message_button()\n",
    "        \n",
    "        actions = ActionChains(self.driver)\n",
    "        m = 0 \n",
    "\n",
    "        while True:\n",
    "            \n",
    "            try:\n",
    "                self.driver.find_element_by_xpath('.//div/div[1]/div/aside/nav').click()\n",
    "                sleep(0.5)\n",
    "                self.driver.find_element_by_xpath('.//div/div[1]/div/aside/nav').click()\n",
    "                sleep(1)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            actions.send_keys(Keys.END)\n",
    "            actions.perform()\n",
    "            sleep(1)\n",
    "\n",
    "            soup = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "            sleep(1)\n",
    "            number_of_messages = len(soup.select('div [class=\"D(f) Fx($flx1) Ov(h) Fxd(c) Pend(10px)\"]'))\n",
    "\n",
    "            if (number_of_messages != m):\n",
    "                m = number_of_messages\n",
    "            else:\n",
    "                return number_of_messages\n",
    "                break\n",
    "                \n",
    "        self.back_to_swipe()\n",
    "               \n",
    "    def matches_report(self):\n",
    "        \n",
    "        self.matches_button()\n",
    "        number = self.get_number_of_matches()\n",
    "        result = []\n",
    " \n",
    "        for i in range (1, math.ceil(number/9)+1):\n",
    "            for h in range(1, 10):\n",
    "                if (i == 1) and (h == 1):\n",
    "                    pass\n",
    "                else:\n",
    "                    try:\n",
    "                        self.driver.find_element_by_xpath(f'.//div[{i}]/div[{h}]/a/div[1]/div').click()\n",
    "                        sleep(1)\n",
    "                        valores = self.prepare()\n",
    "                        sleep(randint(1,3))\n",
    "                        result.append(valores)\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "        self.back_to_swipe()\n",
    "    \n",
    "        return result\n",
    "\n",
    "    def messages_report(self):\n",
    "        \n",
    "        self.message_button()\n",
    "        number = self.get_number_of_messages()\n",
    "        result = []\n",
    "\n",
    "        for i in range(1, number+1):\n",
    "            try:\n",
    "                self.driver.find_element_by_xpath(f'.//div[2]/a[{i}]/div[2]/div[1]').click()\n",
    "                sleep(1)\n",
    "                valores = self.prepare()\n",
    "                sleep(randint(1,3))\n",
    "                result.append(valores)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        self.back_to_swipe()\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def get_my_bio(self):\n",
    "        \n",
    "        try:\n",
    "            #Click my profile\n",
    "            self.driver.find_element_by_xpath('.//div/div[1]/div/aside/div/a/h2').click()\n",
    "            sleep(3.1)\n",
    "            my_bio = self.get_description(BeautifulSoup(self.driver.page_source, 'lxml'))\n",
    "            sleep(3.1)\n",
    "            self.driver.find_element_by_xpath('.//div/div[1]/div/aside/div/div/a').click()\n",
    "            return my_bio\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def check_missing_photos(self, photo_len, tag):\n",
    "        \n",
    "        path, dirs, files = next(os.walk(f\"./images/matches/{tag}\"))\n",
    "        file_count = len(files)\n",
    "        \n",
    "        missing = photo_len - file_count\n",
    "\n",
    "        if missing == 0:\n",
    "            print('All photos were scraped correctly!')\n",
    "            return file_count\n",
    "        else:\n",
    "            print(f'{missing} photos are missing')\n",
    "            return file_count\n",
    "        \n",
    "    \n",
    "    def create_num_face(self, tag):\n",
    "        \n",
    "        files = []\n",
    "        face_images = []\n",
    "        face_files = []\n",
    "\n",
    "        for r, d, f in os.walk(f\"./images/matches/{tag}\"):\n",
    "            for file in f:\n",
    "                if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                    files.append(f\"./images/matches/{tag}/{file}\")\n",
    "\n",
    "        for i in range(len(files)):        \n",
    "\n",
    "            imagePath = sys.argv[1]\n",
    "            cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "            faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "            photo = f\"{files[i]}\"\n",
    "\n",
    "            image = cv2.imread(photo)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            faces = faceCascade.detectMultiScale(\n",
    "                gray,\n",
    "                scaleFactor=1.2,\n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            \n",
    "            basewidth = 224\n",
    "\n",
    "            if int(len(faces)) == 1:\n",
    "                face_files.append(files[i])\n",
    "                for (x, y, w, h) in faces: \n",
    "                    landmarks = image[y:y + h, x:x + w]\n",
    "                    \n",
    "                    height, width, channels = landmarks.shape\n",
    "                    wpercent = (basewidth/float(width))\n",
    "                    hsize = int((float(height)*float(wpercent)))\n",
    "                    \n",
    "                    output = cv2.resize(landmarks, (basewidth, hsize), fx=0.5, fy=0.5, interpolation = cv2.INTER_AREA)\n",
    "                    cv2.imwrite(f'{files[i]}',output) \n",
    "                    \n",
    "            else:\n",
    "                os.remove(files[i])\n",
    "\n",
    "        return len(face_files)\n",
    "    \n",
    "    def attractiveness_score(self, tag):\n",
    "\n",
    "        files = []\n",
    "        for r, d, f in os.walk(f\"./images/matches/{tag}\"):\n",
    "            for file in f:\n",
    "                if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                    files.append(f\"./images/matches/{tag}/{file}\")\n",
    "                    \n",
    "        pixels = []\n",
    "        for i in files:                    \n",
    "            img = image.load_img(i, grayscale=False, target_size=(224, 224)) \n",
    "            x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "            pixels.append(x)\n",
    "\n",
    "        features = []\n",
    "        for i in range(len(pixels)):\n",
    "            features.append(pixels[i])\n",
    "\n",
    "        features = np.array(features)\n",
    "        features = features.reshape(features.shape[0], 224, 224, 3)\n",
    "        features = features / 255\n",
    "\n",
    "        return round(sum(attractiveness_model.predict(features)/len(features))[0],2)\n",
    "    \n",
    "    def is_flag_emoji(self, c):\n",
    "        \n",
    "        return \"\\U0001F1E6\\U0001F1E8\" <= c <= \"\\U0001F1FF\\U0001F1FC\" or c in [\"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f\", \"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f\", \"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f\"]\n",
    "\n",
    "    def deEmojify(self, text):\n",
    "        \n",
    "        regrex_pattern = re.compile(pattern = \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  \n",
    "            u\"\\U0001F680-\\U0001F6FF\"  \n",
    "            u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                               \"]+\", flags = re.UNICODE)\n",
    "        \n",
    "        return regrex_pattern.sub(r'',text)\n",
    "\n",
    "    def give_emoji_free_text(self, text):\n",
    "        \n",
    "        allchars = [str for str in text]\n",
    "        emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI['en']]\n",
    "        clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "        return clean_text\n",
    "    \n",
    "    def tokenization(self, text):    \n",
    "        \n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        try:\n",
    "            token = []\n",
    "            stops = stopwords.words('english')\n",
    "\n",
    "            emojis = list(set([c for c in text if c in emoji.UNICODE_EMOJI['en']]))\n",
    "            flag_emojis = [c for c in text if self.is_flag_emoji(c) == True]\n",
    "            flags = list(set([i+j for i,j in zip(flag_emojis[::2], flag_emojis[1::2])]))\n",
    "            not_text = emojis + flags\n",
    "\n",
    "            text = self.deEmojify(text)\n",
    "            clean_text = self.give_emoji_free_text(text).replace(',', '').replace('.', '').replace(':','').lower().split()\n",
    "\n",
    "            for i in clean_text:\n",
    "                word, tag = pos_tag(word_tokenize(i))[0]\n",
    "                wntag = tag[0].lower()\n",
    "                wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "\n",
    "                if not wntag:\n",
    "                    lemma = word\n",
    "                    if lemma not in stops:\n",
    "                        token.append(lemma) \n",
    "                else:\n",
    "                    lemma = lemmatizer.lemmatize(word, wntag)\n",
    "                    if lemma not in stops:\n",
    "                        token.append(lemma)\n",
    "\n",
    "            return list(set(token))+emojis+flags\n",
    "    \n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    def get_bigrams(self, bio):\n",
    "        \n",
    "        translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "        bigram_meas = BigramAssocMeasures()\n",
    "\n",
    "        bio_translated = translator.translate(bio).text\n",
    "        words = self.tokenization(bio_translated)\n",
    "        \n",
    "        bio_finder = BigramCollocationFinder.from_words(words)\n",
    "        bio_scored = bio_finder.score_ngrams(bigram_meas.raw_freq)\n",
    "\n",
    "        bg = list(map(lambda x: x[0][0] + '_' + x[0][1], bio_scored))\n",
    "        bio_scores = list(map(lambda x: x[1], bio_scored))\n",
    "        bigrams = list(zip(bg, bio_scores))\n",
    "\n",
    "        return bigrams\n",
    "\n",
    "    def cosine_similarity(self, a, b):\n",
    "        \n",
    "        vec1 = Counter(a)\n",
    "        vec2 = Counter(b)\n",
    "\n",
    "        intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "        numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "        sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "        sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "        denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "        if not denominator:\n",
    "            return 0.0\n",
    "        \n",
    "        return float(numerator) / denominator\n",
    "                                 \n",
    "    def match_report(self, links, name, age, location, distance, education, school, job, orientation, likes, s_likes, desc, p_len, spotify, artist, song, verif, sup_like, insta, tag, atr_score, bio_score, profile_score):\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "        color = colors()\n",
    "        \n",
    "        atr_score = (atr_score/5)*2\n",
    "        bio_score = bio_score*1.75\n",
    "        profile_score = profile_score*1.25\n",
    "        \n",
    "        score_general = atr_score + bio_score + profile_score \n",
    "        \n",
    "        print('------------------------------------------------')\n",
    "        data = [[bio_score, atr_score, profile_score, score_general]]\n",
    "        df = pd.DataFrame(data, columns=[\"bio_score\", \"photos_score\", \"profile_score\", \"ovr_score\"])    \n",
    "\n",
    "        fig = plt.figure(figsize=(3,1))\n",
    "\n",
    "        if score_general > 2.35:\n",
    "            ax = fig.add_subplot(1, 2, 1)\n",
    "            img = mpimg.imread('like.jpg')\n",
    "            imgplot = ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            ax = fig.add_subplot(1, 2, 1)\n",
    "            img = mpimg.imread('dislike.jpg')\n",
    "            imgplot = ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, 2)\n",
    "        ax.text(0.1, 0.85,print(df), clip_on=True)\n",
    "        ax.axis('off')\n",
    "\n",
    "        print('------------------------------------------------')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "        self.plot_images(p_len, tag)\n",
    "\n",
    "        print(f\"{color.BOLD}Face photos: {color.END}{p_len}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Name: {color.END}{name}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Age:{color.END} {age}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Location: {color.END}{location}\")\n",
    "        print(f\"{color.BOLD}Distance (km): {color.END}{distance}\")\n",
    "        print(f\"{color.BOLD}School: {color.END}{school}\")\n",
    "        print(f\"{color.BOLD}Job: {color.END}{job}\")\n",
    "        print(f\"{color.BOLD}Orientation: {color.END}{orientation}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Similar likes: {color.END} {s_likes}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Likes: {color.END} {likes}\")\n",
    "        print()\n",
    "\n",
    "        print(f\"{color.BOLD}Description {color.END}\")\n",
    "        print(desc)\n",
    "        print()\n",
    "\n",
    "        print(f'{color.BOLD}Verified:{color.END} {verif}')\n",
    "        print()\n",
    "\n",
    "        print(f'{color.BOLD}Super liked me:{color.END} {sup_like}')\n",
    "        print()\n",
    "\n",
    "        print(f'{color.BOLD}Instagram:{color.END} {insta}')\n",
    "        print()\n",
    "\n",
    "        print(f'{color.BOLD}Spotify:{color.END} {spotify}')\n",
    "        print(f'{color.BOLD}Artist:{color.END} {artist}')\n",
    "        print(f'{color.BOLD}Song:{color.END} {song}')\n",
    "        print()\n",
    "        \n",
    "        return score_general\n",
    "    \n",
    "    def prepare(self, my_bio):\n",
    "        \n",
    "        points = 0\n",
    "        self.back_to_swipe()\n",
    "        self.info_button()\n",
    "        sleep(5)\n",
    "        \n",
    "        actions = ActionChains(self.driver)\n",
    "        sopa = BeautifulSoup(self.driver.page_source, 'lxml')\n",
    "        sleep(0.3)\n",
    "        \n",
    "        photo_len = self.get_photo_len(sopa)\n",
    "        \n",
    "        sleep(0.5)\n",
    "        name = self.get_name(sopa)\n",
    "        sleep(0.1)\n",
    "        age = self.get_age(sopa)\n",
    "        sleep(0.1)\n",
    "        informacion = self.get_info(sopa)\n",
    "        sleep(0.7)\n",
    "\n",
    "        info = dict.fromkeys(['location', 'distance', 'education', 'school', 'job', 'orientation'], None)\n",
    "\n",
    "        for i in range(len(informacion)):\n",
    "            if 'Lives in' in informacion[i]:\n",
    "                info['location'] = informacion[i].replace('Lives in ','')\n",
    "            if 'kilometers away' in informacion[i]:\n",
    "                info['distance'] = informacion[i].replace('kilometers away','')\n",
    "            if education in str(sopa.select('div [class=\"Row\"]')[i]):\n",
    "                info['school'] = sopa.select('div [class=\"Row\"]')[i].text\n",
    "            if work in str(sopa.select('div [class=\"Row\"]')[i]):\n",
    "                info['job'] = sopa.select('div [class=\"Row\"]')[i].text\n",
    "            if gender in str(sopa.select('div [class=\"Row\"]')[i]):\n",
    "                info['orientation'] = sopa.select('div [class=\"Row\"]')[i].text\n",
    "\n",
    "        location = info['location']\n",
    "        \n",
    "        distance = info['distance']\n",
    "        if int(distance) <= 10:\n",
    "            points+=1\n",
    "            \n",
    "        school = info['school']\n",
    "        job = info['job']\n",
    "        orientation = info['orientation']\n",
    "\n",
    "        description = self.get_description(sopa)\n",
    "        likes = self.get_likes(sopa)\n",
    "        \n",
    "        similar_likes = self.get_similar_likes(sopa)\n",
    "        if similar_likes != None:\n",
    "            points+=1\n",
    "        \n",
    "        sleep(0.1)\n",
    "        \n",
    "        now = datetime.now()\n",
    "        current_time = now.strftime(\"%H-%M-%S\")\n",
    "        id_tag = f\"{name[0]}-{age}_{current_time}\"\n",
    "\n",
    "        photo_links = self.scrap_photos(photo_len)\n",
    "        sleep(2)\n",
    "        self.save_images(photo_links, id_tag)\n",
    "        sleep(2)\n",
    "        \n",
    "        new_photo_len = self.check_missing_photos(photo_len, id_tag)\n",
    "        if new_photo_len > 3:\n",
    "            points+=1\n",
    "        \n",
    "        new_photo_len = self.create_num_face(id_tag)\n",
    "        \n",
    "        verif = self.verified(sopa)\n",
    "        \n",
    "        if verif == True:\n",
    "            points+=1\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "        sup_like = self.super_like(sopa)\n",
    "        insta = self.get_instagram(sopa)\n",
    "\n",
    "        music = dict.fromkeys(['spotify', 'artist', 'song'], False)\n",
    "\n",
    "        if self.get_spotify(sopa) == True:\n",
    "            music['spotify'] = True\n",
    "            music['artist'] = self.get_artist(sopa)\n",
    "            music['song'] = self.get_song(sopa)\n",
    "\n",
    "        spotify = music['spotify']\n",
    "        artist = music['artist']\n",
    "        song = music['song']\n",
    "\n",
    "        if not(spotify or insta):\n",
    "            pass\n",
    "        else:\n",
    "            points+=1\n",
    "        \n",
    "        desc_score = self.cosine_similarity(my_bio, self.get_bigrams(description))\n",
    "        atr_score = self.attractiveness_score(id_tag)\n",
    "        profile_score = points/5\n",
    "        \n",
    "        ovr_score = self.match_report(photo_links, name, age, location, distance, education, school, job, orientation, likes, similar_likes, description, new_photo_len, spotify, artist, song, verif, sup_like, insta, id_tag, atr_score, desc_score, profile_score)\n",
    "        self.delete_images(id_tag)\n",
    "        \n",
    "        self.info_out()\n",
    "\n",
    "        if ovr_score > 2.15:\n",
    "            self.like_button()\n",
    "            self.no_super_like()\n",
    "            self.close_match()\n",
    "        else:\n",
    "            self.dislike_button()\n",
    "\n",
    "        return name, age, new_photo_len, school, job, orientation, location, distance, similar_likes, likes, description, verif, sup_like, insta, spotify, artist, song, atr_score, desc_score, id_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bot swiping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = TinderBot()\n",
    "bot.login()\n",
    "df = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_bio = bot.get_my_bio()\n",
    "my_bigrams = bot.get_bigrams(my_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_match = bot.prepare(my_bio)\n",
    "this_df = pd.DataFrame(list([new_match]), columns = cols)\n",
    "df = df.append(this_df, ignore_index = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# while True:\n",
    "#     new_match = bot.prepare(my_bio)\n",
    "#     this_df = pd.DataFrame(list([new_match]), columns = cols)\n",
    "#     df = df.append(this_df, ignore_index = True)\n",
    "#     df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attractiveness_score(tag):\n",
    "\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(f\"./images/matches/{tag}\"):\n",
    "                for file in f:\n",
    "                    if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                        files.append(f\"./images/matches/{tag}/{file}\")\n",
    "    #Retriveing pixels\n",
    "    pixels = []\n",
    "    for i in files:                    \n",
    "        img = image.load_img(i, grayscale=False, target_size=(224, 224)) \n",
    "        x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "        pixels.append(x)\n",
    "\n",
    "    #Features\n",
    "    features = []\n",
    "    for i in range(len(pixels)):\n",
    "        features.append(pixels[i])\n",
    "\n",
    "    features = np.array(features)\n",
    "    features = features.reshape(features.shape[0], 224, 224, 3)\n",
    "    features = features / 255\n",
    "    \n",
    "    return round(sum(attractiveness_model.predict(features)/len(features))[0],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractiveness_score('D-25_22-39-05')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attractiveness_model = load_model(\"ratings.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = ['name','age', 'number_of_photos', 'education', 'work', 'sexual_orientation', 'location','km_distance','similar_likes','other_likes','description','verified','super_liked_me','instagram','spotify','artist','song', 'attractiveness_score', 'description_score', 'folder']\n",
    "# new_df = pd.DataFrame(list([new_match]), columns = cols)\n",
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #bot.get_number_of_messages()\n",
    "\n",
    "# msgs = bot.messages_report()\n",
    "# df_msgs = pd.DataFrame(msgs, columns = cols)\n",
    "# df_msgs.to_csv('messages.csv', index = False)\n",
    "# df_msgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# #print(bot.get_number_of_matches())\n",
    "\n",
    "# mtchs = bot.matches_report()\n",
    "# df_mtchs = pd.DataFrame(mtchs, columns = cols)\n",
    "# df_mtchs.to_csv('matches.csv', index = False)\n",
    "# df_mtchs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from keras.preprocessing import image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import sys\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from mlxtend.image import extract_face_landmarks\n",
    "import itertools\n",
    "from sklearn import linear_model\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking correct scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_path = f\"{os.getcwd()}/images/messages\"\n",
    "matches_path = f\"{os.getcwd()}/images/matches\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileNames(target, source):\n",
    "    files = []\n",
    "    file_count = 0\n",
    "    path = f\"{source}/%s/\" % (target)\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f: \n",
    "            if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                files.append(file)\n",
    "    return files\n",
    "\n",
    "def check_missing_photos(df, source):\n",
    "    \n",
    "    df[\"files\"] = df.folder.apply(getFileNames, args=[source])\n",
    "    df['file_count'] = df['files'].str.len()\n",
    "    missing = sum(df['number_of_photos'] - df['file_count'])\n",
    "    \n",
    "    if missing == 0:\n",
    "        df = df.drop(columns=['file_count', 'files'], inplace = True)\n",
    "        print('All photos were scraped correctly!')\n",
    "    else:\n",
    "        print(f'{missing} photos are missing. There was a problem scraping photos in the following folders:\\n')\n",
    "        df['missing'] = df['number_of_photos'] - df['file_count']\n",
    "        print(df[(df['number_of_photos'] - df['file_count'] > 0) == True][['folder', 'missing']])\n",
    "\n",
    "        while True:\n",
    "            ans = ['y', 'n']    \n",
    "            inp = input('Do you want to continue working with you current photos? (y/n)')\n",
    "\n",
    "            if inp.lower() in ans:        \n",
    "                if inp == 'y':\n",
    "                    df['number_of_photos'] = df['file_count']\n",
    "                    df = df.drop(columns = ['file_count', 'files', 'missing'], inplace = True)\n",
    "                    print('Photo lenght succesfully modified')\n",
    "                    break\n",
    "                else:\n",
    "                    print('Scrap the remaining photos or decide what you want to do before continuing')\n",
    "                    break\n",
    "            else:\n",
    "                print('Please provide a valid answer (y/n)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_msgs = pd.read_csv('messages.csv')\n",
    "# cols = ['name','age', 'number_of_photos', 'education', 'work', 'sexual_orientation', 'location','km_distance','similar_likes','other_likes','description','verified','super_liked_me','instagram','spotify','artist','song', 'attractiveness_score', 'description_score', 'folder']\n",
    "# df_msgs.columns = cols\n",
    "# df_msgs['messaged_me'] = 1\n",
    "df_msgs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_missing_photos(df_msgs, messages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mtchs = pd.read_csv('matches.csv')\n",
    "# df_mtchs.columns = cols\n",
    "# df_mtchs['messaged_me'] = 0\n",
    "df_mtchs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_missing_photos(df_mtchs, matches_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if face appears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_face(tag):\n",
    "\n",
    "    files = []\n",
    "    face_images = []\n",
    "    face_files = []\n",
    "\n",
    "    for r, d, f in os.walk(f\"./images/matches/{tag}\"):\n",
    "        for file in f:\n",
    "            if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                files.append(f\"./images/matches/{tag}/{file}\")\n",
    "                \n",
    "    for i in range(len(files)):        \n",
    "        \n",
    "        imagePath = sys.argv[1]\n",
    "        cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "        photo = f\"{files[i]}\"\n",
    "\n",
    "        image = cv2.imread(photo)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        if int(len(faces)) == 1:\n",
    "            face_files.append(files[i])\n",
    "            face_images.append(mpimg.imread(f\"{files[i]}\"))\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    data = {'files':face_files}\n",
    "    df = pd.DataFrame(data)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    columns = len(face_images)\n",
    "    \n",
    "    for i, image in enumerate(face_images):\n",
    "        plt.subplot(int(len(face_images)/columns+1), columns, i+1)\n",
    "        plt.imshow(image)\n",
    "        plt.gca().axes.get_yaxis().set_visible(False)\n",
    "        plt.gca().axes.get_xaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tag = 'D-25_14-38-49'\n",
    "\n",
    "create_num_face(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_num_face(path, tag):\n",
    "\n",
    "    face_check = []\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(f\"{path}\"):\n",
    "        for file in f:\n",
    "            if ('.jpg' in file) or ('.jpeg' in file) or '.png' in file:\n",
    "                files.append(file)\n",
    "\n",
    "    for i in range(len(files)):        \n",
    "        \n",
    "        imagePath = sys.argv[1]\n",
    "        cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "        photo = f\"{path}/{tag}/{files[i]}\"\n",
    "\n",
    "        image = cv2.imread(photo)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        faces = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "        \n",
    "        face_check.append(int(len(faces)))\n",
    "\n",
    "    data = {'files':files, 'face': face_check}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# faces_msg = create_num_face(messages_path)\n",
    "# faces_msg = faces_msg[faces_msg['face'] == 1]\n",
    "\n",
    "# faces_matches = create_num_face(matches_path)\n",
    "# faces_matches = faces_matches[faces_matches['face'] == 1]\n",
    "\n",
    "# df_faces = pd.concat([faces_msg, faces_matches])\n",
    "#df_faces.to_csv('df_faces.csv')\n",
    "\n",
    "df_faces = pd.read_csv('df_faces.csv')\n",
    "df_faces.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labeling faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_labels = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "def check_valid_label(filename):\n",
    "    try:\n",
    "        inp = int(input('What\\'s the score?'))\n",
    "        if inp not in valid_labels:\n",
    "            print(\"results: status must be one of %r.\" % valid_labels)\n",
    "            inp = check_valid_label(filename)\n",
    "        else:\n",
    "            print(f'Label saved for file: {filename}')\n",
    "    except:\n",
    "        print(\"results: status must be one of %r.\" % valid_labels)\n",
    "        inp = check_valid_label(filename)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "def create_face_labels(path, csv_name):\n",
    "\n",
    "    labels = []\n",
    "    folder = []\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file in list(df_faces['files']):\n",
    "                files.append(file)\n",
    "                folder.append(r.split('/')[-1])\n",
    "\n",
    "    folder_path = [f\"{path}/{folder[i]}\" for i in range(len(folder))]\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        im = PIL.Image.open(f\"{folder_path[i]}/{files[i]}\")\n",
    "        plt.imshow(im)\n",
    "        plt.show()\n",
    "        \n",
    "        inp = check_valid_label(files[i])\n",
    "        labels.append(int(inp))\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    data = {'files':files, 'folder':folder, 'ratings':labels}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f'{csv_name}.csv', index = False) \n",
    "\n",
    "    print(f'\\n{csv_name}.csv saved!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg_face_ratings = create_face_labels(messages_path, 'message_face_ratings')\n",
    "# msg_face_ratings['messaged_me'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg_face_ratings = pd.merge(pd.read_csv('message_face_labels.csv').drop(columns=['label']), pd.read_csv('message_labels.csv').set_index('folder'))\n",
    "# msg_face_ratings.columns = ['files', 'folder', 'ratings']\n",
    "# msg_face_ratings['messaged_me'] = 1\n",
    "# msg_face_ratings.to_csv('msg_face_ratings.csv', index=False)\n",
    "msg_face_ratings = pd.read_csv('msg_face_ratings.csv')\n",
    "msg_face_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# matches_face_ratings = create_face_labels(matches_path, 'matches_face_ratings')\n",
    "# matches_face_ratings['messaged_me'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matches_face_ratings = pd.merge(pd.read_csv('matches_face_labels.csv').drop(columns=['label']), pd.read_csv('matches_labels.csv').set_index('folder'))\n",
    "# matches_face_ratings.columns = ['files', 'folder', 'ratings']\n",
    "# matches_face_ratings['messaged_me'] = 0\n",
    "# matches_face_ratings.to_csv('matches_face_ratings.csv', index=False)\n",
    "matches_face_ratings = pd.read_csv('matches_face_ratings.csv')\n",
    "matches_face_ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.concat([msg_face_ratings, matches_face_ratings])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_faces(path):\n",
    "\n",
    "    folder = []\n",
    "    files = []\n",
    "\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file in list(df_faces['files']):\n",
    "                files.append(file)\n",
    "                folder.append(r.split('/')[-1])\n",
    "\n",
    "    folder_path = [f\"{path}/{folder[i]}\" for i in range(len(folder))]\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        imagePath = sys.argv[1]\n",
    "        cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "        faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "        image = cv2.imread(f\"{folder_path[i]}/{files[i]}\")\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        landmarks = faceCascade.detectMultiScale(\n",
    "            gray,\n",
    "            scaleFactor=1.2,\n",
    "            minNeighbors=5,\n",
    "            minSize=(30, 30)\n",
    "        )\n",
    "    \n",
    "        for (x, y, w, h) in landmarks: \n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 0, 255), 2) \n",
    "            landmarks = image[y:y + h, x:x + w] \n",
    "            lugar = f\"{os.getcwd()}/cropped_faces/\"\n",
    "            cv2.imwrite(os.path.join(lugar , f'{files[i]}'), landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_faces(messages_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_faces(matches_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering non-faces and resizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = f\"{os.getcwd()}/cropped_faces/\"\n",
    "\n",
    "# width = []\n",
    "# heigth = []\n",
    "\n",
    "# for i in range(len(train_df['files'])):\n",
    "#     image = PIL.Image.open(f\"{path}{train_df['files'][i]}\")\n",
    "#     w, h = image.size\n",
    "#     width.append(w)\n",
    "#     heigth.append(h)\n",
    "    \n",
    "# avg_width = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filtering_non_faces(csv_name):\n",
    "\n",
    "    basewidth = 224\n",
    "    files = []\n",
    "    features = []\n",
    "    face_files = []\n",
    "\n",
    "    path = f\"{os.getcwd()}/cropped_faces/\"\n",
    "\n",
    "    for r, d, f in os.walk(path):\n",
    "        for file in f:\n",
    "            if file in list(df_faces['files']):\n",
    "                files.append(f\"{file}\")\n",
    "\n",
    "    for i in range(len(files)):\n",
    "        img = imageio.imread(f\"{path}{files[i]}\")\n",
    "        landmarks = extract_face_landmarks(img)\n",
    "\n",
    "        if type(landmarks) == type(None):\n",
    "            os.remove(f\"{path}{files[i]}\")\n",
    "            pass\n",
    "        else:\n",
    "            features.append(landmarks)\n",
    "            face_files.append(files[i])\n",
    "            img = PIL.Image.open(f\"{path}{files[i]}\")\n",
    "            wpercent = (basewidth/float(img.size[0]))\n",
    "            hsize = int((float(img.size[1])*float(wpercent)))\n",
    "            img = img.resize((basewidth, hsize), PIL.Image.ANTIALIAS)\n",
    "            img.save(f\"{path}{files[i]}\")\n",
    "          \n",
    "    data = {'files':face_files, 'landmarks':features}\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(f'{csv_name}.csv', index = False) \n",
    "\n",
    "    print(f'\\n{csv_name}.csv saved!')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_faces = filtering_non_faces('only_faces')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_faces = pd.read_csv('only_faces.csv')\n",
    "filtered_faces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = filtered_faces.merge(training, on=['files'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train_df[train_df['messaged_me'] == 1].groupby('folder', as_index=False).agg({'files':'count', 'ratings':'sum'})\n",
    "\n",
    "# temp_df = df_msgs.merge(train_df[train_df['messaged_me'] == 1].groupby('folder', as_index=False).agg({'files':'count', 'ratings':'sum'}))\n",
    "# temp_df['attractiveness_score'] = round(temp_df['ratings']/temp_df['files'],2)\n",
    "# temp_df.drop(columns=['ratings', 'files'], inplace = True)\n",
    "# temp_df = temp_df[['attractiveness_score', 'folder']]\n",
    "\n",
    "# df_msgs = pd.merge(df_msgs, temp_df,  on ='folder', how ='left').drop(columns=['attractiveness_score_x']).rename(columns={\"attractiveness_score_y\": \"attractiveness_score\"})\n",
    "# df_msgs.to_csv('messages.csv')\n",
    "# df_msgs.head(1)\n",
    "df_msgs = pd.read_csv('messages.csv')\n",
    "df_msgs.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[train_df['messaged_me'] == 0].groupby('folder', as_index=False).agg({'files':'count', 'ratings':'sum'})\n",
    "\n",
    "# temp_df = df_mtchs.merge(train_df[train_df['messaged_me'] == 0].groupby('folder', as_index=False).agg({'files':'count', 'ratings':'sum'}))\n",
    "# temp_df['attractiveness_score'] = round(temp_df['ratings']/temp_df['files'],2)\n",
    "# temp_df.drop(columns=['ratings', 'files'], inplace = True)\n",
    "# temp_df = temp_df[['attractiveness_score', 'folder']]\n",
    "\n",
    "# df_mtchs = pd.merge(df_mtchs, temp_df,  on ='folder', how ='left').drop(columns=['attractiveness_score_x']).rename(columns={\"attractiveness_score_y\": \"attractiveness_score\"})\n",
    "# df_mtchs.to_csv('matches.csv')\n",
    "# df_mtchs.head(1)\n",
    "df_mtchs = pd.read_csv('matches.csv')\n",
    "df_mtchs.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievePixels(path):\n",
    "    img = image.load_img(path, grayscale=False, target_size=(224, 224)) \n",
    "    x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Input, Convolution2D, ZeroPadding2D, MaxPooling2D, Activation\n",
    "from keras.layers import Conv2D, AveragePooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrievePixels(path):\n",
    "    img = image.load_img(path, grayscale=False, target_size=(224, 224)) \n",
    "    x = image.img_to_array(img).reshape(1, -1)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_deep = train_df.copy()\n",
    "df_deep['f_path'] = f'{os.getcwd()}/cropped_faces/' + df_deep['files']\n",
    "df_deep['pixels'] = df_deep['f_path'].progress_apply(retrievePixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, instance in df_deep.sort_values(by=['ratings'], ascending = False).head(3).iterrows():\n",
    "    img = instance.pixels\n",
    "    img = img.reshape(224, 224, 3)\n",
    "    img = img / 255\n",
    "    \n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "    print(instance.files)\n",
    "    print(\"Attractiveness score: \", instance.ratings)\n",
    "    \n",
    "    print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "pixels = df_deep['pixels'].values\n",
    "for i in range(0, pixels.shape[0]):\n",
    "    features.append(pixels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "#features = features.reshape(features.shape[0], 224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, df_deep.ratings.values, test_size=0.2, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set: \", X_train.shape[0])\n",
    "print(\"Test set: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG-Face model\n",
    "base_model = Sequential()\n",
    "base_model.add(ZeroPadding2D((1,1),input_shape=(224,224, 3)))\n",
    "base_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(256, (3, 3), activation='relu'))\n",
    "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(ZeroPadding2D((1,1)))\n",
    "base_model.add(Convolution2D(512, (3, 3), activation='relu'))\n",
    "base_model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    " \n",
    "base_model.add(Convolution2D(4096, (7, 7), activation='relu'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Convolution2D(4096, (1, 1), activation='relu'))\n",
    "base_model.add(Dropout(0.5))\n",
    "base_model.add(Convolution2D(2622, (1, 1)))\n",
    "base_model.add(Flatten())\n",
    "base_model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.load_weights('./data/vgg_face_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'ratings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model_output = Sequential()\n",
    "base_model_output = Flatten()(base_model.layers[-4].output)\n",
    "base_model_output = Dense(num_of_classes)(base_model_output)\n",
    "\n",
    "attractiveness_model = Model(inputs=base_model.input, outputs=base_model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractiveness_model.compile(loss='mean_squared_error', optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(\n",
    "    filepath='%s.hdf5' % (target)\n",
    "    , monitor = \"val_loss\"\n",
    "    , verbose=1\n",
    "    , save_best_only=True\n",
    "    , mode = 'auto'\n",
    ")\n",
    "\n",
    "earlyStop = EarlyStopping(monitor='val_loss', patience=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = attractiveness_model.fit(\n",
    "    X_train, y_train\n",
    "    , epochs=5000\n",
    "    , validation_data=(X_test, y_test)\n",
    "    , callbacks=[checkpointer, earlyStop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_iteration = np.argmin(score.history['val_loss'])+1\n",
    "\n",
    "test_scores = score.history['val_loss'][0:best_iteration]\n",
    "train_scores = score.history['loss'][0:best_iteration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_scores, label='val_loss')\n",
    "plt.plot(train_scores, label='train_loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attractiveness_model = load_model(\"%s.hdf5\" % (target))\n",
    "attractiveness_model.save_weights('%s.h5' % (target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = attractiveness_model.predict(X_test)\n",
    "actuals = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf = pd.DataFrame(actuals, columns = [\"actuals\"])\n",
    "perf[\"predictions\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"pearson correlation: \",perf[['actuals', 'predictions']].corr(method ='pearson').values[0,1])\n",
    "print(\"mae: \", mean_absolute_error(actuals, predictions))\n",
    "print(\"rmse: \", sqrt(mean_squared_error(actuals, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_limit = df_deep.ratings.min(); max_limit = df_deep.ratings.max()\n",
    "best_predictions = []\n",
    "\n",
    "#for i in np.arange(1, 7, 0.01):\n",
    "for i in np.arange(int(min_limit), int(max_limit)+1, 0.01):\n",
    "    best_predictions.append(round(i, 2))\n",
    "\n",
    "plt.scatter(best_predictions, best_predictions, s=1, color = 'black', alpha=0.5)\n",
    "plt.scatter(predictions, actuals, s=20, alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import _pickle as pickle\n",
    "from nltk import FreqDist\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "\n",
    "from googletrans import Translator\n",
    "from google_trans_new import google_translator\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "import emoji\n",
    "import flag\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_bigrams('Soy una persona muy directa y que le gusta comer cuando es de noche 🎍😍😍')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(bio):\n",
    "    translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "    bigram_meas = BigramAssocMeasures()\n",
    "    \n",
    "    bio_translated = translator.translate(bio).text\n",
    "    words = tokenization(bio_translated)\n",
    "\n",
    "    bio_finder = BigramCollocationFinder.from_words(words)\n",
    "    bio_scored = bio_finder.score_ngrams(bigram_meas.raw_freq)\n",
    "    \n",
    "    bg = list(map(lambda x: x[0][0] + '_' + x[0][1], bio_scored))\n",
    "    bio_scores = list(map(lambda x: x[1], bio_scored))\n",
    "    bigrams = list(zip(bg, bio_scores))\n",
    "\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msgs = pd.read_csv('messages_tokenized.csv')\n",
    "df_mtchs = pd.read_csv('matches_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = google_translator()\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_msgs['description'] = df_msgs['description'].apply(lambda x: np.nan if type(x) == float else x.replace('\\n',' ').replace('\\r','').replace(',', ' ').replace('.', ' ').replace(':', ' '))\n",
    "# #df_msgs['description'] = df_msgs['description'].apply(lambda x: np.nan if type(x) == float else translator.translate(x).text)\n",
    "# df_msgs['description'] = df_msgs['description'].apply(lambda x: np.nan if type(x) == float else translator.translate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mtchs['description'] = df_mtchs['description'].apply(lambda x: np.nan if type(x) == float else x.replace('\\n','').replace(',', '').replace('.', ' ').replace(':', ' '))\n",
    "# #df_mtchs['description'] = df_mtchs['description'].apply(lambda x: np.nan if type(x) == float else translator.translate(x).text)\n",
    "# df_mtchs['description'] = df_mtchs['description'].apply(lambda x: np.nan if type(x) == float else translator.translate(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def is_flag_emoji(c):\n",
    "    return \"\\U0001F1E6\\U0001F1E8\" <= c <= \"\\U0001F1FF\\U0001F1FC\" or c in [\"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0065\\U000e006e\\U000e0067\\U000e007f\", \"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0073\\U000e0063\\U000e0074\\U000e007f\", \"\\U0001F3F4\\U000e0067\\U000e0062\\U000e0077\\U000e006c\\U000e0073\\U000e007f\"]\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  \n",
    "        u\"\\U0001F680-\\U0001F6FF\"  \n",
    "        u\"\\U0001F1E0-\\U0001F1FF\" \n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI['en']]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def tokenization(text):    \n",
    "    try:\n",
    "        token = []\n",
    "        stops = stopwords.words('english')\n",
    "        \n",
    "        emojis = list(set([c for c in text if c in emoji.UNICODE_EMOJI['en']]))\n",
    "        flag_emojis = [c for c in text if is_flag_emoji(c) == True]\n",
    "        flags = list(set([i+j for i,j in zip(flag_emojis[::2], flag_emojis[1::2])]))\n",
    "        not_text = emojis + flags\n",
    "        \n",
    "        text = deEmojify(text)\n",
    "        clean_text = give_emoji_free_text(text).replace(',', '').replace('.', '').replace(':','').lower().split()\n",
    "\n",
    "        for i in clean_text:\n",
    "            word, tag = pos_tag(word_tokenize(i))[0]\n",
    "            wntag = tag[0].lower()\n",
    "            wntag = wntag if wntag in ['a', 'r', 'n', 'v'] else None\n",
    "            \n",
    "            if not wntag:\n",
    "                lemma = word\n",
    "                if lemma not in stops:\n",
    "                    token.append(lemma) \n",
    "            else:\n",
    "                lemma = lemmatizer.lemmatize(word, wntag)\n",
    "                if lemma not in stops:\n",
    "                    token.append(lemma)\n",
    "                            \n",
    "        return list(set(token))+emojis+flags\n",
    "    \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization(my_bio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_msgs['tokenized'] = df_msgs['description'].apply(lambda x: np.nan if type(x) == float else tokenization(x))\n",
    "# df_mtchs['tokenized'] = df_mtchs['description'].apply(lambda x: np.nan if type(x) == float else tokenization(x))\n",
    "\n",
    "# df_msgs.to_csv('messages_tokenized.csv')\n",
    "# df_mtchs.to_csv('matches_tokenized.csv')\n",
    "\n",
    "# df_msgs = pd.read_csv('messages_tokenized.csv')\n",
    "# df_mtchs = pd.read_csv('matches_tokenized.csv')\n",
    "\n",
    "# df_msgs.to_csv('messages_tokenized.csv')\n",
    "# df_mtchs.to_csv('matches_tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df_msgs, df_mtchs])\n",
    "df.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set()\n",
    "\n",
    "for bio in df['tokenized']:\n",
    "    try:\n",
    "        res = bio.replace(\"'\",\"\").strip('][').split(', ')\n",
    "        total_vocab.update(res)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Number of unique words: \",len(total_vocab))\n",
    "\n",
    "words = []\n",
    "\n",
    "for bio in df['tokenized']:\n",
    "    try:\n",
    "        res = bio.replace(\"'\",\"\").strip('][').split(', ')\n",
    "        words.extend(res)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "bio_freq = FreqDist(words)\n",
    "bio_freq.most_common(509)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.bar(*zip(*bio_freq.most_common(10)))\n",
    "plt.xticks(rotation=75)\n",
    "plt.title('Top 10 Most Frequently Used Words in my Tinder Matches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_meas = BigramAssocMeasures()\n",
    "bio_finder = BigramCollocationFinder.from_words(words)\n",
    "bio_scored = bio_finder.score_ngrams(bigram_meas.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bg = list(map(lambda x: x[0][0] + '_' + x[0][1], bio_scored[:10]))\n",
    "bio_scores = list(map(lambda x: x[1], bio_scored[:10]))\n",
    "bigrams = list(zip(bg, bio_scores))\n",
    "\n",
    "plt.style.use('bmh')\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.bar(*zip(*bigrams[:10]))\n",
    "plt.xticks(rotation=80)\n",
    "plt.title('Top 10 Bigramas más comúnes en mis matches')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = px.bar(bigrams, x=bg, y=bio_scores, height=400, labels=dict(x=\"Bigrams\", y=\"%\"))\n",
    "fig.update_traces(marker_color='green')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.io as pio\n",
    "pio.write_image(fig, 'bigrams.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_finder.apply_freq_filter(20)\n",
    "bio_pmi = bio_finder.score_ngrams(bigram_meas.pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_pmi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msgs.description.apply(lambda bio: np.nan if type(bio) == float else BigramCollocationFinder.from_words(bio).nbest(bigram_meas.pmi, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text = \" \".join([(k + \" \")*v for k,v in bio_freq.items()])\n",
    "\n",
    "wordcloud = WordCloud(collocations=False, max_font_size=40).generate(text)\n",
    "\n",
    "fig = plt.figure(figsize = (20, 10), facecolor = 'k', edgecolor = 'k')\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    vec1 = Counter(a)\n",
    "    vec2 = Counter(b)\n",
    "    \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    return float(numerator) / denominator\n",
    "\n",
    "\n",
    "def text_to_vector(text):\n",
    "    word = re.compile(r'\\w+')\n",
    "    words = word.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "\n",
    "def get_result(content_a, content_b):\n",
    "    text1 = content_a\n",
    "    text2 = content_b\n",
    "\n",
    "    vector1 = text_to_vector(text1)\n",
    "    vector2 = text_to_vector(text2)\n",
    "\n",
    "    cosine_result = get_cosine(vector1, vector2)\n",
    "    return cosine_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get Tuple algorithms \n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams # This is the ngram magic.\n",
    "from textblob import TextBlob\n",
    "\n",
    "NGRAM = 2\n",
    "\n",
    "re_sent_ends_naive = re.compile(r'[.\\n]')\n",
    "re_stripper_alpha = re.compile('[^a-zA-Z]+')\n",
    "re_stripper_naive = re.compile('[^a-zA-Z\\.\\n]')\n",
    "\n",
    "splitter_naive = lambda x: re_sent_ends_naive.split(re_stripper_naive.sub(' ', x))\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def get_tuples_nosentences(txt):\n",
    "    \"\"\"Get tuples that ignores all punctuation (including sentences).\"\"\"\n",
    "    if not txt: return None\n",
    "    ng = ngrams(re_stripper_alpha.sub(' ', txt).split(), NGRAM)\n",
    "    return list(ng)\n",
    "\n",
    "def get_tuples_manual_sentences(txt):\n",
    "    \"\"\"Naive get tuples that uses periods or newlines to denote sentences.\"\"\"\n",
    "    if not txt: return None\n",
    "    sentences = (x.split() for x in splitter_naive(txt) if x)\n",
    "    ng = (ngrams(x, NGRAM) for x in sentences if len(x) >= NGRAM)\n",
    "    return list(chain(*ng))\n",
    "\n",
    "def get_tuples_nltk_punkt_sentences(txt):\n",
    "    \"\"\"Get tuples that doesn't use textblob.\"\"\"\n",
    "    if not txt: return None\n",
    "    sentences = (re_stripper_alpha.split(x) for x in sent_detector.tokenize(txt) if x)\n",
    "    # Need to filter X because of empty 'words' from punctuation split\n",
    "    ng = (ngrams(filter(None, x), NGRAM) for x in sentences if len(x) >= NGRAM)\n",
    "    return list(chain(*ng))\n",
    "\n",
    "def get_tuples_textblob_sentences(txt):\n",
    "    \"\"\"New get_tuples that does use textblob.\"\"\"\n",
    "    if not txt: return None\n",
    "    tb = TextBlob(txt)\n",
    "    ng = (ngrams(x.words, NGRAM) for x in tb.sentences if len(x.words) > NGRAM)\n",
    "    return [item for sublist in ng for item in sublist]\n",
    "\n",
    "def jaccard_distance(a, b):\n",
    "    \"\"\"Calculate the jaccard distance between sets A and B\"\"\"\n",
    "    a = set(a)\n",
    "    b = set(b)\n",
    "    return 1.0 * len(a&b)/len(a|b)\n",
    "\n",
    "def cosine_similarity_ngrams(a, b):\n",
    "    vec1 = Counter(a)\n",
    "    vec2 = Counter(b)\n",
    "    \n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    return float(numerator) / denominator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_tuples_nosentences(\"I'm not looking for sex\")\n",
    "b = get_tuples_nosentences(\"Only looking for sex\")\n",
    "print(\"Jaccard: {}   Cosine: {}\".format(jaccard_distance(a,b), cosine_similarity_ngrams(a,b)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \"I think I'm a very attractive person that looks for the best in people\"\n",
    "\n",
    "manual = get_tuples_manual_sentences(texto)\n",
    "non = get_tuples_nosentences(texto)\n",
    "nltk = get_tuples_nltk_punkt_sentences(texto)\n",
    "blob = get_tuples_textblob_sentences(texto)\n",
    "print(f'Manual: {manual}')\n",
    "print(f'None: {non}')\n",
    "print(f'NLTK: {nltk}')\n",
    "print(f'Blob: {blob}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/miguelcampero/Desktop/'\n",
    "file = '1588347020_775480_1588347098_noticia_normal.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath = sys.argv[1]\n",
    "cascPath = \"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Create the haar cascade\n",
    "faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "\n",
    "# Read the image\n",
    "image = cv2.imread(path+file)\n",
    "rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\n",
    "# Detect faces in the image\n",
    "faces = faceCascade.detectMultiScale(\n",
    "    gray,\n",
    "    scaleFactor=1.9,\n",
    "    minNeighbors=5,\n",
    "    minSize=(30, 30),\n",
    ")\n",
    "\n",
    "\n",
    "for (x, y, w, h) in faces: \n",
    "        cv2.rectangle(gray, (x, y), (x+w, y+h), (0, 255, 0), 10) \n",
    "        cv2.rectangle(rgb, (x, y), (x+w, y+h), (0, 255, 0), 10)\n",
    "        faces = image[y:y + h, x:x + w] \n",
    "        plt.imshow(rgb, cmap = plt.cm.Spectral)\n",
    "        cv2.imwrite('prueba2.jpg', cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 10))\n",
    "        plt.axis('off')\n",
    "        cv2.imwrite('prueba.jpg', faces)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '02.jpg'\n",
    "img = imageio.imread(path)\n",
    "landmarks = extract_face_landmarks(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax.axis('off')\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "ax.imshow(img)\n",
    "ax.axis('off')\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "ax.scatter(landmarks[:, 0], -landmarks[:, 1], alpha=0.8, color='black')\n",
    "ax.axis('off')\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "img2 = img.copy()\n",
    "\n",
    "for p in landmarks:\n",
    "    img2[p[1]-3:p[1]+3, p[0]-3:p[0]+3, :] = (255, 255, 255)\n",
    "    \n",
    "ax.imshow(img2)\n",
    "ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('rates.txt', train_df['label'], fmt='%i', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = []\n",
    "\n",
    "for i in range(len(train_df['landmarks'])):\n",
    "    save.append(train_df['landmarks'][i].reshape(-1,1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('testout.txt', save, fmt='%i', delimiter=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.loadtxt('testout.txt', delimiter=',', usecols=range(136))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GENERATE FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def facialRatio(points):\n",
    "    x1 = points[0];\n",
    "    y1 = points[1];\n",
    "    x2 = points[2];\n",
    "    y2 = points[3];\n",
    "    x3 = points[4];\n",
    "    y3 = points[5];\n",
    "    x4 = points[6];\n",
    "    y4 = points[7];\n",
    "\n",
    "    dist1 = math.sqrt((x1-x2)**2 + (y1-y2)**2)\n",
    "    dist2 = math.sqrt((x3-x4)**2 + (y3-y4)**2)\n",
    "\n",
    "    ratio = dist1/dist2\n",
    "\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateFeatures(pointIndices1, pointIndices2, pointIndices3, pointIndices4, allLandmarkCoordinates):\n",
    "    \n",
    "    size = allLandmarkCoordinates.shape\n",
    "    allFeatures = np.zeros((size[0], len(pointIndices1)))\n",
    "    for x in range(0, size[0]):\n",
    "        landmarkCoordinates = allLandmarkCoordinates[x, :]\n",
    "        ratios = [];\n",
    "        for i in range(0, len(pointIndices1)):\n",
    "            x1 = landmarkCoordinates[2*(pointIndices1[i]-1)]\n",
    "            y1 = landmarkCoordinates[2*pointIndices1[i] - 1]\n",
    "            x2 = landmarkCoordinates[2*(pointIndices2[i]-1)]\n",
    "            y2 = landmarkCoordinates[2*pointIndices2[i] - 1]\n",
    "\n",
    "            x3 = landmarkCoordinates[2*(pointIndices3[i]-1)]\n",
    "            y3 = landmarkCoordinates[2*pointIndices3[i] - 1]\n",
    "            x4 = landmarkCoordinates[2*(pointIndices4[i]-1)]\n",
    "            y4 = landmarkCoordinates[2*pointIndices4[i] - 1]\n",
    "\n",
    "            points = [x1, y1, x2, y2, x3, y3, x4, y4]\n",
    "            ratios.append(facialRatio(points))\n",
    "        allFeatures[x, :] = np.asarray(ratios)\n",
    "    return allFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAllFeatures(allLandmarkCoordinates):\n",
    "    a = [18, 22, 23, 27, 37, 40, 43, 46, 28, 32, 34, 36, 5, 9, 13, 49, 55, 52, 58]\n",
    "    combinations = itertools.combinations(a, 4)\n",
    "    i = 0\n",
    "    pointIndices1 = [];\n",
    "    pointIndices2 = [];\n",
    "    pointIndices3 = [];\n",
    "    pointIndices4 = [];\n",
    "\n",
    "    for combination in combinations:\n",
    "        pointIndices1.append(combination[0])\n",
    "        pointIndices2.append(combination[1])\n",
    "        pointIndices3.append(combination[2])\n",
    "        pointIndices4.append(combination[3])\n",
    "        i = i+1\n",
    "        pointIndices1.append(combination[0])\n",
    "        pointIndices2.append(combination[2])\n",
    "        pointIndices3.append(combination[1])\n",
    "        pointIndices4.append(combination[3])\n",
    "        i = i+1\n",
    "        pointIndices1.append(combination[0])\n",
    "        pointIndices2.append(combination[3])\n",
    "        pointIndices3.append(combination[1])\n",
    "        pointIndices4.append(combination[2])\n",
    "        i = i+1\n",
    "\n",
    "    return generateFeatures(pointIndices1, pointIndices2, pointIndices3, pointIndices4, allLandmarkCoordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks = np.loadtxt(f'testout.txt', delimiter=',', usecols=range(136))\n",
    "featuresALL = generateAllFeatures(landmarks)\n",
    "np.savetxt('ft_ALL.txt', featuresALL, delimiter=',', fmt = '%.04f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('ft_ALL.txt', delimiter=',')\n",
    "ratings = np.loadtxt('rates.txt', delimiter=',')\n",
    "predictions = np.zeros(ratings.size);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(ratings)):\n",
    "    features_train = np.delete(features, i, 0)\n",
    "    features_test = features[i, :].reshape(1,-1)\n",
    "    ratings_train = np.delete(ratings, i, 0)\n",
    "    ratings_test = ratings[i]\n",
    "    pca = decomposition.PCA(n_components=20)\n",
    "    pca.fit(features_train)\n",
    "    features_train = pca.transform(features_train)\n",
    "    features_test = pca.transform(features_test)\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(features_train, ratings_train)\n",
    "    predictions[i] = regr.predict(features_test)\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('cross_valpred.txt', predictions, delimiter=',', fmt = '%.04f')\n",
    "corr = np.corrcoef(predictions, ratings)[0, 1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.loadtxt('ft_ALL.txt', delimiter=',')\n",
    "features_train = features[0:-50]\n",
    "features_test = features[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=20)\n",
    "pca.fit(features_train)\n",
    "features_train = pca.transform(features_train)\n",
    "features_test = pca.transform(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = np.loadtxt(f'rates.txt', delimiter=',')\n",
    "ratings_train = ratings[0:-50]\n",
    "ratings_test = ratings[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(features_train, ratings_train)\n",
    "ratings_predict = regr.predict(features_test)\n",
    "corr = np.corrcoef(ratings_predict, ratings_test)[0, 1]\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residue = np.mean((ratings_predict - ratings_test) ** 2)\n",
    "print(residue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rangeArray = np.arange(1, 51)\n",
    "plt.plot(rangeArray, ratings_test, 'r', rangeArray, ratings_predict, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Messages attractiveness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_labels = msg_labels.groupby('folder').agg({'label':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "msg_result = pd.concat([msg_labels.set_index('folder'), df_msgs.set_index('folder')], axis=1, join='inner').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_result[\"messaged_me\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_result['attractiveness_score'] = (msg_result['label']/msg_result['number_of_photos']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_result = msg_result.drop(columns=['index','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg_result.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matches attractiveness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_labels = pd.read_csv('matches_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "match_labels = match_labels.groupby('folder').agg({'label':'sum'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matches_result = pd.concat([match_labels.set_index('folder'), df_mtchs.set_index('folder')], axis=1, join='inner').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_result[\"messaged_me\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_result['attractiveness_score'] = (matches_result['label']/matches_result['number_of_photos']).round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_result = matches_result.drop(columns=['index','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_result.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#         for i in list(set(token)):\n",
    "#             final_lemma = str()\n",
    "#             for j in i:\n",
    "#                 if j not in emojis:\n",
    "#                     if j not in flags:\n",
    "#                         final_lemma += j\n",
    "#             not_text.append(final_lemma)\n",
    "\n",
    "        #return list(set([i for i in not_text if i])) \n",
    "        #return list(set(token))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
